from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators import PostgresOperator
from airflow.operators import CreateStagingFolder, DestroyStagingFolder
from airflow.utils.slack import slack_failed_alert, slack_success_alert
from datetime import datetime, timedelta


# ============================================================
# Defaults - these arguments apply to all operators

default_args = {
    'owner': 'airflow',  # TODO: Look up what owner is
    'depends_on_past': False,  # TODO: Look up what depends_on_past is
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2019, 2, 4, 0, 0, 0),
    'on_failure_callback': slack_failed_alert,
    'on_success_callback': slack_success_alert,
    # 'queue': 'bash_queue',  # TODO: Lookup what queue is
    # 'pool': 'backfill',  # TODO: Lookup what pool is
}

pipeline = DAG('etl_hash_test_v0', schedule_interval='0 5 * * *', default_args=default_args)

# ------------------------------------------------------------
# Make staging area

make_staging = CreateStagingFolder(
    task_id='make_hash_test_staging',
    dag=pipeline,
)

# ------------------------------------------------------------
# Extract - read files from LNI

extract_builtingcodes = GeopetlReadOperator(
    task_id='read_buildingcodes',
    dag=pipeline,
    csv_path='{{ ti.xcom_pull("make_hash_test_staging") }}/buildingcoedes.csv',
    db_conn_id='brt-viewer',
    db_table_name='brt_admin.buildingcodes',
    db_table_where='',
)

# ----------------------------------------------------
# Write extracted files to Databridge

write_buildingcodes = GeopetlWriteOperator(
    task_id='write_buildingcodes',
    dag=pipeline,
    csv_path='{{ ti.xcom_pull("make_hash_test_staging") }}/buildingcodes.csv',
    db_conn_id='databridge2',
    db_table_name='opa.buildingcodes',
)

# ----------------------------------------------------
# Update Hash
hash_fields_stmt = '''
    SELECT array_agg(COLUMN_NAME::text order by COLUMN_NAME)
	FROM information_schema.columns
	WHERE table_schema='opa' AND table_name='buildingcodes' 
	and column_name not like 'etl%'
'''

get_hash_fields = PostgresOperator(
    task_id='get_hash_fields',
    dag=pipeline,
    postgres_conn_id='databridge2',
    database='databridge_raw',
    sql=hash_fields_stmt,

)

# -----------------------------------------------------------------
# Cleanup - delete staging folder

cleanup = DestroyStagingFolder(
    task_id='cleanup_staging',
    dag=pipeline,
    dir='{{ ti.xcom_pull("make_hash_test_staging") }}',
)

extract_buildingcodes.set_upstream(make_staging)
extract_buildingcodes.set_downstream(write_buildingcodes)
write_buildingcodes.set_downstream(cleanup)
